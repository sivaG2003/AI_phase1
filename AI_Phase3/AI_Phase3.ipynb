{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b78bc2a-eacd-4537-967c-26fc29723406",
   "metadata": {},
   "source": [
    "![FAKE NEWS DETECTION!](https://engineering.ucdavis.edu/sites/g/files/dgvnsk2151/files/styles/sf_landscape_16x9/public/media/images/413f7eda-5991-660f-a0e9-36d5a7ee2754.png?h=e1043f6d&itok=0jrdgkk5 \"Fake news detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6cd59f-b772-4690-b514-c1fc54ab254f",
   "metadata": {},
   "source": [
    "# **Download The Dataset**\n",
    "### - Go to the Kaggle dataset page: **https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset.**\r",
    "### - \tDownload the dataset files, which typically come in the form of CSV or other common formats.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8a620-961e-4290-bb9d-551d2f1d428e",
   "metadata": {},
   "source": [
    "# **Import Necessary Libraries**\n",
    "### - In Python, you'll want to import libraries that you'll use for data manipulation and machine learning.\n",
    "### - **Common libraries include pandas, numpy, scikit-learn, and NLTK (Natural Language Toolkit).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83294610-196c-48fb-abb4-be35fc78d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204d79d-20dd-48eb-86a1-ace89d7e2587",
   "metadata": {},
   "source": [
    "# **Load the Data**\n",
    "### **Read the downloaded dataset files into pandas dataframes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4cbf66-6eb0-445e-986e-16d250f085b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have downloaded 'fake.csv' and 'true.csv' files\n",
    "fake_news_df = pd.read_csv('C:\\\\Users\\\\Abdul\\\\Downloads\\\\archive\\\\fake.csv')\n",
    "real_news_df = pd.read_csv('C:\\\\Users\\\\Abdul\\\\Downloads\\\\archive\\\\true.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba40c7-dc02-4caa-bb73-b272723ab7c5",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "* Combine the fake and real news data.\n",
    "* \tLabel the data as 'fake' and 'real' for classification\n",
    "* \tRemove any unnecessary column\n",
    "* \tHandle missing values if necessay\n",
    "* \t\tClean and preprocess the textual data (text of the news articles)\n",
    "\r\n",
    "Here's an example of preprocessing for text data using Python's NLTK library:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7365cc-da58-400f-982f-24b66dfb9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and non-alphabetic characters, and lemmatize the words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "fake_news_df['text'] = fake_news_df['text'].apply(preprocess_text)\n",
    "real_news_df['text'] = real_news_df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7533ded-1752-42f1-8093-06aedf730fa6",
   "metadata": {},
   "source": [
    "# Preprocess the Data\n",
    "- Preprocessing steps typically include:\n",
    "  * **Text Cleaning:** *Remove any unnecessary characters, symbols, and HTML tags from the text.*\n",
    "  * **Lowercasing:** *IConvert all text to lowercase to ensure uniformity.*\n",
    "  * **Tokenization:** *Split the text into individual words or tokens.*\n",
    "  * **Stopword Removal:** *Remove common stopwords like \"the,\" \"and,\" \"in,\" etc.*\n",
    "  * **Vectorization:** *Convert text data into numerical form using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings.*\n",
    "  * **Label Encoding:** *If your dataset contains labels (real or fake news), you may need to encode them into numerical values (e.g., 0 for fake, 1 for real).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a93c5a-8a1a-4daf-9cb6-630e84936c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning and lowercasing\n",
    "fake_news_data['text'] = fake_news_data['text'].str.replace('[^\\w\\s]', '').str.lower()\n",
    "\n",
    "# Tokenization and stopword removal (using NLTK)\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "fake_news_data['text'] = fake_news_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(fake_news_data['text'])\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(fake_news_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a33bb1-3df6-4fdd-abf7-d60321ad7bd9",
   "metadata": {},
   "source": [
    "# Combine and Shuffle Data:\n",
    "- Combine the fake and real news data.\n",
    "- \tShuffle the data to ensure it's not ordered.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb5aaa-67c7-4032-8dec-65376fce96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data\n",
    "data = pd.concat([fake_news_df, real_news_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=TrueP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
