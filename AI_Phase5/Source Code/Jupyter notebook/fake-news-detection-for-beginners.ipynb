{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNosEPLZsx0f"
   },
   "source": [
    "## **What Is Fake News ?**\n",
    "\n",
    "False news, also known as unwanted news, false stories, misconceptions or fraudulent stories, types of stories that contain deliberate information or frauds that are spread through traditional media (print and broadcast) or online media. Digital news has revived and increased the use of fake news, or yellow journalism. These stories are often referred to as information that is not in the media but sometimes finds its way into the mainstream media as well.\n",
    "\n",
    "\n",
    "*   It causes panic\n",
    "*   Damaging the reputation of public and private organizations\n",
    "*   It deceives the people, for the benefit of the deceivers\n",
    "*   Motivated by a personal vendetta, some people support such things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWC8Yl8NrTVI"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3T8_C3DNQ60_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIn0nTNJrjr4"
   },
   "source": [
    "**Loading The Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Tkti84VFRgRE"
   },
   "outputs": [],
   "source": [
    "f = pd.read_csv('Dataset/Fake.csv', delimiter = ',')\n",
    "t = pd.read_csv('Dataset/True.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NfaHxWErrYG"
   },
   "source": [
    "**Observing the top 5 rows of fake and true data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "51oll5ylSMTF",
    "outputId": "c8e86cc7-5e91-4a73-830e-918a73a7ae88"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HZZWrFQ2SOFz",
    "outputId": "8ba64586-2ced-4334-8b70-701723b77a9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roS1x0u6r2Df"
   },
   "source": [
    "Now combining these two dataset to one dataset to simplify processing.\n",
    "\n",
    "Also to combine we need to add an extra column as 'temp' to differtiate news as 1=true_news 0=fake_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fDj4cVR6RwVg"
   },
   "outputs": [],
   "source": [
    "f['temp']= 0\n",
    "t['temp']= 1\n",
    "\n",
    "datas = pd.DataFrame()\n",
    "#datas = t.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFIygCDpxkc_"
   },
   "source": [
    "**Observing the top 5 rows of dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mcU3j-ITxsL5",
    "outputId": "2e5b4742-d216-43f0-cb59-dc0fe0765f0e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQy7wL8SwgBu"
   },
   "source": [
    "**Checking the dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GTcT2G9ywldw",
    "outputId": "cf17fadc-1a43-416d-d59f-b1231215f87a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(datas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGdvzlK5sC5g"
   },
   "source": [
    "**Column 'Date' and 'Subject' are important to Descriptive analysis but here for prediction they are less important so dropping these columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eldrd8KyR4kw"
   },
   "outputs": [],
   "source": [
    "column = ['date','subject']\n",
    "#datas = datas.drop(columns=column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDx7GKAfwp2n"
   },
   "source": [
    "**Cheking the dimensions after dropping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "u4e-c_ZIwr73",
    "outputId": "c238f182-fb41-49d3-e26f-c72c4110ffdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(datas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzvDyXoywx2_"
   },
   "source": [
    "**Exploring the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "P65xxII6ww05",
    "outputId": "067aec9f-b573-4deb-90c2-4234d2d2479d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n"
     ]
    }
   ],
   "source": [
    "datas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MII5G82zsLCO"
   },
   "source": [
    "**Created array of 'title' column as input_array for preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7jWE09tYsJe4"
   },
   "outputs": [],
   "source": [
    "#input_arr=np.array(datas['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-oQumWTsRuP"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIZ_Lv4H1S9n"
   },
   "source": [
    "*Downloading Stopwords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "HdoXk8PmSB7i",
    "outputId": "f1c5a7ff-98bd-4edd-dbb5-f41af4ee2f33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5zXvujG1OgF"
   },
   "source": [
    "***Stopwords***: *A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.*\n",
    "https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "\n",
    "***PortStemmer***: *A PortStemmer is an algorithm used for removing the commoner morphological and inflexional endings from words in English.  For example: words such as “Likes”, ”liked”, ”likely” and ”liking” will be reduced to “like” after stemming.*\n",
    "https://www.geeksforgeeks.org/python-stemming-words-with-nltk/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4BuoPqdFzQV"
   },
   "source": [
    "**Using nltk and importing Stopwords(For dealing with stopwords) and PortStemmer(For stemming)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qbQE4S45Sktw"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m corpus \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m40000\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     newArr \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43minput_arr\u001b[49m[i])\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#The sub() function replaces the matches with the text of your choice,\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#in this case \"[a-zA-Z]\" is getting replaced with blank space\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#input_arr[i] is the array of 'title' column.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#[a-zA-Z]: Returns a match for any character alphabetically between a and z, lower case OR upper case\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     newArr \u001b[38;5;241m=\u001b[39m newArr\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_arr' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, 40000):\n",
    "    newArr = re.sub('[^a-zA-Z]', ' ', input_arr[i])\n",
    "    #The sub() function replaces the matches with the text of your choice,\n",
    "    #in this case \"[a-zA-Z]\" is getting replaced with blank space\n",
    "    #input_arr[i] is the array of 'title' column.\n",
    "\n",
    "    #[a-zA-Z]: Returns a match for any character alphabetically between a and z, lower case OR upper case\n",
    "\n",
    "    newArr = newArr.lower()\n",
    "    #Converting into lowercase\n",
    "\n",
    "    newArr = newArr.split()\n",
    "    #The split() method splits a string into a list.\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    newArr = [ps.stem(word) for word in newArr if not word in set(stopwords.words('english'))]\n",
    "    #ps.stem(word) is stemming the words\n",
    "    #the word will be considered if the word is not a stopword\n",
    "    #set(stopwords.words('english')= checks any kind of stopwords in English language\n",
    "\n",
    "    newArr = ' '.join(newArr)\n",
    "    #joins the string with blank spaces\n",
    "\n",
    "    corpus.append(newArr)\n",
    "    #adding strings into the corpus list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:25.458783Z",
     "iopub.status.busy": "2021-11-23T12:58:25.45815Z",
     "iopub.status.idle": "2021-11-23T12:58:25.471316Z",
     "shell.execute_reply": "2021-11-23T12:58:25.470391Z",
     "shell.execute_reply.started": "2021-11-23T12:58:25.458743Z"
    },
    "id": "1RTB_Wxsxro-",
    "outputId": "191642a8-b98b-4d99-fefb-377d0d2c3a19"
   },
   "outputs": [],
   "source": [
    "datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QvVnbr1rCW"
   },
   "source": [
    "***CountVectorizer*** : *CountVectorizer is a tool provided by the scikit-learn library. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.* \n",
    "https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zeW2JtkLSqgg"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m countv \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#max_features: The CountVectorizer will select the\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#words/features/terms which occur the most frequently.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#It takes absolute values so if you set the ‘max_features = 3’,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#it will select the 3 most common words in the data.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcountv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m datas\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m40000\u001b[39m, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m         )\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countv = CountVectorizer(max_features = 5000)\n",
    "#max_features: The CountVectorizer will select the\n",
    "#words/features/terms which occur the most frequently.\n",
    "#It takes absolute values so if you set the ‘max_features = 3’,\n",
    "#it will select the 3 most common words in the data.\n",
    "\n",
    "X = countv.fit_transform(corpus).toarray()\n",
    "y = datas.iloc[0:40000, 2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3ciK92LySSb"
   },
   "source": [
    "X=independent_variable(title)\n",
    "</br> y=dependent_variable(sentiment 0 or 1)\n",
    "\n",
    "</br>\n",
    "Basically what I did here is created a bag of words (https://machinelearningmastery.com/gentle-introduction-bag-words-model/)i.e, X will have top 5000 most common words in array form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWCRpCJytbOX"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L51J2xHhsfuH"
   },
   "source": [
    "**Splitting the dataset into the Training set and Test set in 80-20 ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:26.678349Z",
     "iopub.status.busy": "2021-11-23T12:58:26.678031Z",
     "iopub.status.idle": "2021-11-23T12:58:27.504032Z",
     "shell.execute_reply": "2021-11-23T12:58:27.502913Z",
     "shell.execute_reply.started": "2021-11-23T12:58:26.678309Z"
    },
    "id": "tlGuYEnaU_fu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLGkDV3Ntope"
   },
   "source": [
    "**Fitting Logistic Regression to the Training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:27.505999Z",
     "iopub.status.busy": "2021-11-23T12:58:27.505653Z",
     "iopub.status.idle": "2021-11-23T12:58:40.770939Z",
     "shell.execute_reply": "2021-11-23T12:58:40.770122Z",
     "shell.execute_reply.started": "2021-11-23T12:58:27.505954Z"
    },
    "id": "yiwMSdFFtu4-",
    "outputId": "195af7a4-0304-4526-e47b-863cfa2195c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FACWoKH7tx1X"
   },
   "source": [
    "**Predicting the Test set results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:40.772733Z",
     "iopub.status.busy": "2021-11-23T12:58:40.772198Z",
     "iopub.status.idle": "2021-11-23T12:58:40.95621Z",
     "shell.execute_reply": "2021-11-23T12:58:40.955096Z",
     "shell.execute_reply.started": "2021-11-23T12:58:40.772689Z"
    },
    "id": "vgzLHzcEtzmq"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-1ZNDIkt5vY"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6fOn_TeuPbO"
   },
   "source": [
    "**Making the Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:40.958409Z",
     "iopub.status.busy": "2021-11-23T12:58:40.957737Z",
     "iopub.status.idle": "2021-11-23T12:58:40.985661Z",
     "shell.execute_reply": "2021-11-23T12:58:40.984767Z",
     "shell.execute_reply.started": "2021-11-23T12:58:40.95836Z"
    },
    "id": "PVgM-6qrt1dB"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:40.989041Z",
     "iopub.status.busy": "2021-11-23T12:58:40.988235Z",
     "iopub.status.idle": "2021-11-23T12:58:41.000889Z",
     "shell.execute_reply": "2021-11-23T12:58:40.999986Z",
     "shell.execute_reply.started": "2021-11-23T12:58:40.988991Z"
    },
    "id": "ntqdcCNRuZ94",
    "outputId": "373f3486-cc30-4f02-9d9e-03477d562090"
   },
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWRIZAk3v5uN"
   },
   "source": [
    "**Getting more details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:41.008992Z",
     "iopub.status.busy": "2021-11-23T12:58:41.006224Z",
     "iopub.status.idle": "2021-11-23T12:58:41.046588Z",
     "shell.execute_reply": "2021-11-23T12:58:41.045652Z",
     "shell.execute_reply.started": "2021-11-23T12:58:41.008932Z"
    },
    "id": "nL__ptq2ucmM",
    "outputId": "3e5fad6f-9de7-4227-f8bb-2d15db7cadad"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs2rmUBiy6Sp"
   },
   "source": [
    "TP – True Positives\n",
    "TN - True Negatives\n",
    "FP – False Positives\n",
    "FN – False Negatives\n",
    "\n",
    "Precision: Accuracy of positive predictions.\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    "Recall: Fraction of positives that were correctly identified.\n",
    "Recall = TP/(TP+FN)\n",
    "\n",
    "F1 score – Percent of positive predictions were correct.\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q-iYjLiIzZ-"
   },
   "source": [
    "**Getting Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:41.052671Z",
     "iopub.status.busy": "2021-11-23T12:58:41.051782Z",
     "iopub.status.idle": "2021-11-23T12:58:41.066928Z",
     "shell.execute_reply": "2021-11-23T12:58:41.065993Z",
     "shell.execute_reply.started": "2021-11-23T12:58:41.052617Z"
    },
    "id": "fpqeGK4TIdD-",
    "outputId": "64236d97-05fa-4642-af6e-db3f4e033185"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuarcy: {}\".format(round(accuracy_score(y_test, y_pred)*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZCED1Onv7-e"
   },
   "source": [
    "**Confusion matrix in a well plotted chart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:41.073129Z",
     "iopub.status.busy": "2021-11-23T12:58:41.072413Z",
     "iopub.status.idle": "2021-11-23T12:58:41.2115Z",
     "shell.execute_reply": "2021-11-23T12:58:41.210887Z",
     "shell.execute_reply.started": "2021-11-23T12:58:41.073076Z"
    },
    "id": "4fEFCDDwvJQD"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T12:58:41.213646Z",
     "iopub.status.busy": "2021-11-23T12:58:41.212925Z",
     "iopub.status.idle": "2021-11-23T12:58:41.441613Z",
     "shell.execute_reply": "2021-11-23T12:58:41.440766Z",
     "shell.execute_reply.started": "2021-11-23T12:58:41.213607Z"
    },
    "id": "_sU_egMPveut",
    "outputId": "8a72c8a2-c512-475d-8017-8681241e0d2d"
   },
   "outputs": [],
   "source": [
    "new_cm = pd.DataFrame(cm , index = ['Fake','Not Fake'] , columns = ['Fake','Not Fake'])\n",
    "sns.heatmap(new_cm,cmap= 'Blues', annot = True, fmt='',xticklabels = ['Fake','Not Fake'], yticklabels = ['Fake','Not Fake'])\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title('Confusion matrix On Test Data')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
